{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle to save space\n",
    "mode = '_mean' if False else '_all'\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbm_params = {\n",
    "    'n_estimators' : 10, # 500,  \n",
    "    'max_depth' : 3,\n",
    "    'learning_rate': 0.9,\n",
    "    'bagging_fraction': 0.8, # TODO: try 0.9\n",
    "    \n",
    "    'feature_fraction' : 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'subsample' : 0.1,  # \n",
    "    'subsample_freq' : 1,\n",
    "    'num_leaves' : 20,\n",
    "    'metric':'rmse',\n",
    "    #'lambda_l1' : 1,  # Try defaults\n",
    "    #'lambda_l2': 1, # Try defaults\n",
    "    'verbose': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, TransformerMixin\n",
    "import gc\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.core.dtypes.dtypes import CategoricalDtype\n",
    "from tqdm import tqdm\n",
    "from datetime import date \n",
    "import holidays\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# label encoding\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ConvertToDatetime(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_dtype = {\n",
    "    'train': {'building_id': np.int16, 'meter': np.int8, 'meter_reading': np.float32},\n",
    "    'test': {'building_id': np.int16, 'meter': np.int8},\n",
    "    'building_metadata': {'site_id': np.int8, 'building_id': np.uint16, 'square_feet': np.float32, 'year_built': np.float16, 'floor_count': np.float16},\n",
    "}\n",
    "\n",
    "def loadFile(name):\n",
    "    for dir_path in ['../input/ashrae-energy-prediction/','../input/_ashrae-energy-prediction/']:\n",
    "        if path.exists(dir_path + name + '.csv'):\n",
    "            return  ConvertToDatetime().transform(\n",
    "                pd.read_csv(dir_path + name + '.csv', dtype=file_dtype[name]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "building = loadFile('building_metadata')\n",
    "pre_train = loadFile('train')\n",
    "#test = loadFile('test')\n",
    "\n",
    "weather_processed_df = pd.read_pickle(f'../input/ashrae-energy-prediction-pickles/weather_processed{mode}.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(x):\n",
    "    weather_processed_df = pd.read_pickle(f'../input/ashrae-energy-prediction-pickles/weather_processed{mode}.pickle')\n",
    "    x = x.merge(building, on=['building_id'], how='left')\n",
    "    gc.collect()\n",
    "    x = x.merge(weather_processed_df, on=['site_id', 'timestamp'], how='left')\n",
    "    gc.collect()\n",
    "    x['site_id'] = x['site_id'].astype('int8');\n",
    "    x['cloud_coverage'] = x['cloud_coverage'].astype('float16')\n",
    "    gc.collect()\n",
    "    return x\n",
    "        \n",
    "train = merge(pre_train)  \n",
    "print(train)\n",
    "print('!!!! Warning we are missing weather for '+ str(train['air_temperature'].isnull().sum())+' rows')\n",
    "train = train.dropna(axis=0, subset=['air_temperature'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See holiday notebook to generate, this is optional\n",
    "holiday_df = None\n",
    "if path.exists('../input/ashrae-energy-prediction-pickles/holiday_df.pickle'):\n",
    "    holiday_df = pd.read_pickle('../input/ashrae-energy-prediction-pickles/holiday_df.pickle')\n",
    "if holiday_df is not None:\n",
    "    print(holiday_df.sample(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MeterReadingLog1p(TransformerMixin):\n",
    "  \n",
    "    def transform(self, df, **transform_params):\n",
    "        if 'meter_reading' in df.columns:\n",
    "            df['meter_reading_log1p'] = np.log1p(df['meter_reading'])\n",
    "            df = df.drop('meter_reading', axis=1)\n",
    "        return df\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "print(train.sample(20, random_state=42))\n",
    "print(MeterReadingLog1p().transform(train.sample(20, random_state=42)))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AddTimeFeatures(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df_a, **transform_params):\n",
    "        # TODO: try week of year as numerical \n",
    "        df = df_a\n",
    "        df['dayofweek'] = df['timestamp'].dt.dayofweek.astype('category') # vs weekend?\n",
    "        #df['weekday'] = df['timestamp'].dt.weekday.astype('category')\n",
    "        #df['dayofweek_hour'] = (df['timestamp'].dt.dayofweek * 24) + df['timestamp'].dt.hour\n",
    "        #df['dayofweek_hour'] = df['dayofweek_hour'].astype('category')\n",
    "        #df['week'] = df['timestamp'].dt.week.astype('category')\n",
    "        df['hour'] = df['timestamp'].dt.hour.astype('uint8')\n",
    "        return df\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meter_desc_columns={'mean': 'meter_mean', 'max': 'meter_max', 'min': 'meter_min', 'std':'meter_std'}\n",
    "\n",
    "class CreateMeterDescDF(TransformerMixin):\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        global _building_meter_desc_DF\n",
    "        print(df.columns)\n",
    "        if 'meter_reading_log1p' in df.columns:\n",
    "            cols = ['mean']\n",
    "            if mode == '_all':\n",
    "                cols = ['mean','max','min','std']\n",
    "            group = df.groupby(['building_id','meter'])['meter_reading_log1p']\n",
    "            desc_DF = group.describe()[cols]\n",
    "            desc_DF = desc_DF.reset_index()\n",
    "            _building_meter_desc_DF = desc_DF.rename(columns=meter_desc_columns)\n",
    "            gc.collect()\n",
    "        return df \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "#if 'meter_mean' not in train.columns:\n",
    "#    print(building_meter_desc_DF)\n",
    "#    train = train.merge(building_meter_desc_DF, on=['building_id','meter'], how='left')\n",
    "#    #test = test.merge(building_meter_desc_DF, on=['building_id','meter'], how='left')\n",
    "#    del building_meter_desc_DF\n",
    "CreateMeterDescDF().transform(\n",
    "    AddTimeFeatures().transform(\n",
    "        MeterReadingLog1p().transform(\n",
    "            train.sample(2000, random_state=0)\n",
    "        )\n",
    "    )\n",
    ")    \n",
    "print(_building_meter_desc_DF)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeMeterDescDF(TransformerMixin):\n",
    "  \n",
    "    def transform(self, df, **transform_params):\n",
    "        # drop any columns to add\n",
    "        df = df.drop(meter_desc_columns.values(), axis=1, errors='ignore') \n",
    "        return df.merge(_building_meter_desc_DF, on=['building_id','meter'], how='left')\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "print(MergeMeterDescDF().transform(train.sample(2000, random_state=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"As you can see above, this data looks weired until May 20. It is \n",
    "# reported in this discussion by @barnwellguy that All electricity\n",
    "# meter is 0 until May 20 for site_id == 0. Let's remove these data \n",
    "# from training data.\"\n",
    "# https://www.kaggle.com/kaushal2896/ashrae-eda-fe-lightgbm-1-13\n",
    "class RmS0M0(TransformerMixin):\n",
    "  \n",
    "    def transform(self, df, **transform_params):\n",
    "        return df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: write filter to remove any 0 meter reading that continue more than N days (try 3)\n",
    "# Also we need to account for this by meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: try rolling with power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "# https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114483#latest-660771\n",
    "# https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114874#latest-660970\n",
    "class AddHolidays(TransformerMixin):\n",
    "    def transform(self, df, **transform_params):\n",
    "        if holiday_df is not None:\n",
    "            if 'holiday' in df.columns:\n",
    "                df = df.drop('holiday', axis=1)\n",
    "            df = df.merge(holiday_df, on=['timestamp','site_id'], how='left')\n",
    "            df['holiday'] = df['holiday'].astype('category')\n",
    "        else:\n",
    "            print(\"Warning: Holiday DF is missing\")\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "# Test \n",
    "if holiday_df is not None:\n",
    "    print(holiday_df.columns)\n",
    "    print(AddHolidays().transform(train.head(20))[['holiday','timestamp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RmHolidays(TransformerMixin):\n",
    "    def transform(self, df, **transform_params):\n",
    "        if holiday_df is not None:\n",
    "            df = df.merge(holiday_df, on=['timestamp','site_id'], how='left')\n",
    "            df = df.drop(df[df['holiday'].notnull()].index)\n",
    "            df = df.drop(['holiday'], axis=1)\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(\"Warning: Holiday DF is missing\")\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# Test you should see the new years removed\n",
    "#print(train.head(100000).merge(building, on='building_id', how='left').columns)\n",
    "print(RmHolidays().transform(train.head(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SetCatTypes(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols):\n",
    "        self._cols = cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        for col in self._cols:\n",
    "            df[col]= df[col].astype('category')\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropCols(TransformerMixin):\n",
    "\n",
    "    def __init__(self, drop_cols):\n",
    "        self._drop_cols = drop_cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        df = df.drop(self._drop_cols, axis=1)\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LogSquareFeet(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        df['log_square_feet'] = np.float16(np.log(df['square_feet']))\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "print(building.head(20)['square_feet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Play with scaling cloud coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ImputeYearBuilt(TransformerMixin):\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        # revisit the choice of median vs anything else\n",
    "        tmp_df = train.drop_duplicates(['site_id','building_id'])[['site_id','building_id','year_built']]\n",
    "        year_built_median = tmp_df['year_built'].median()\n",
    "        # Set all year_built NaNs to site mean for year_built\n",
    "        for i, i_median in tmp_df.groupby(['site_id'])['year_built'].median().items():\n",
    "            if not np.isnan(i_median):\n",
    "                df.loc[(df['year_built'].isnull()) & (df['site_id'] == i), 'year_built'] = i_median\n",
    "            else:\n",
    "                df.loc[(df['year_built'].isnull()) & (df['site_id'] == i), 'year_built'] = year_built_median\n",
    "        df['building_age'] = np.uint8(df['year_built']-1900)\n",
    "        del tmp_df, year_built_median\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "print(ImputeYearBuilt().transform(train.sample(20))['building_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ImputeFloorCount(TransformerMixin):\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        # revisit the choice of median vs anything else\n",
    "        tmp_df = train.drop_duplicates(['site_id','building_id'])[['site_id','building_id','floor_count']]\n",
    "        floors_median = tmp_df['floor_count'].median()\n",
    "        # Set all year_built NaNs to site mean for year_built\n",
    "        for i, i_median in tmp_df.groupby(['site_id'])['floor_count'].median().items():\n",
    "            if not np.isnan(i_median):\n",
    "                df.loc[(df['floor_count'].isnull()) & (df['site_id'] == i), 'floor_count'] = i_median\n",
    "            else:\n",
    "                df.loc[(df['floor_count'].isnull()) & (df['site_id'] == i), 'floor_count'] = floors_median\n",
    "        del tmp_df, floors_median\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "print(ImputeFloorCount().transform(train.sample(20))['floor_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AddMeterDummies(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df_a, **transform_params):\n",
    "        df = df_a\n",
    "        for i in range(4):\n",
    "            df['_meter_'+str(i)] = (df['building_id'].isin(\n",
    "                train.loc[train['meter'] == i].building_id.unique()))\n",
    "        return df\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AddRelativeHumidity(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df_a, **transform_params):\n",
    "        df = df_a\n",
    "        # code here\n",
    "        return df\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FillMean(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self._cols = cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        for col in self._cols:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FillZeros(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self._cols = cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        for col in self._cols:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FillMedian(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self._cols = cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        for col in self._cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class FillPopular(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self._cols = cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        for col in self._cols:\n",
    "            df[col] = df[col].fillna(df[col].value_counts()[0])\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MarkNaNs(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        for col in  df.columns[df.isna().any()].tolist():\n",
    "            df['_' + col + '_nan' ] = df[col].isnull()\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GC(TransformerMixin):\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        gc.collect()\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return np.sqrt(mean_squared_log_error(y, y_pred.clip(0)))\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return mean_squared_error(y, y_pred.clip(0))\n",
    "\n",
    "def rmsee(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y.clip(0)), np.expm1(y_pred.clip(0))))\n",
    "    \n",
    "rmsle_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsle(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsle(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "rmsee_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsee(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "def lbm_rmsle(y_true, y_pred):\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False\n",
    "\n",
    "# rob's custome function to do RMSLE while in the log1p space\n",
    "def lbm_rmslee(y_true, y_pred):\n",
    "    return 'RMSLEE', np.sqrt(np.mean(np.power(y_pred - y_true, 2))), False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_pre_pipes = Pipeline(\n",
    "    steps=[\n",
    "        ('meterReadingLog1p',MeterReadingLog1p()),\n",
    "        ('rmS0M0', RmS0M0()),\n",
    "        ('addTimeFeatures', AddTimeFeatures()),\n",
    "        ('logSquareFeet', LogSquareFeet()),\n",
    "        #('rmHolidays', RmHolidays()),\n",
    "        #('addHolidays', AddHolidays()),\n",
    "        ('createMeterDescDF', CreateMeterDescDF()), # note declares a globe variable to pass\n",
    "        ('mergeMeterDescDF', MergeMeterDescDF()), # populates both test and train from global\n",
    "        ('setCatTypes', SetCatTypes(['building_id', 'site_id', 'meter', 'primary_use'])),\n",
    "        ('GC', GC())\n",
    "    ]\n",
    ")\n",
    "\n",
    "train = x_pre_pipes.transform(train)\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pre_a_pipes is for preprocessing that doesn't change impute\n",
    "# values\n",
    "x_fold_pipes = Pipeline(\n",
    "    steps=[\n",
    "        #('markNans',MarkNaNs()),\n",
    "        #('convertToDatetime', ConvertToDatetime()),\n",
    "        ('imputeYearBuilt', ImputeYearBuilt()),\n",
    "        ('imputeFloorCount', ImputeFloorCount()),\n",
    "        ('dropCols', DropCols(['timestamp','square_feet', 'year_built','site_id','building_id'])),\n",
    "        ('GC', GC())\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_train_X = x_fold_pipes.transform(train.sample(20))\n",
    "print(sample_train_X.columns)\n",
    "print(sample_train_X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this stratified strategy from\n",
    "# https://www.kaggle.com/isaienkov/lightgbm-fe-1-19/notebook\n",
    "folds = 5\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "best_scores = []\n",
    "for train_index, val_index in kf.split(train, train['building_id']):\n",
    "    f_train_X = x_fold_pipes.transform(train.loc[train_index])\n",
    "    f_val_X = x_fold_pipes.transform(train.loc[val_index])\n",
    "    gbm = LGBMRegressor(**gbm_params)\n",
    "    gbm.fit(f_train_X.drop('meter_reading_log1p', axis=1), f_train_X['meter_reading_log1p'],\n",
    "        eval_set=[(f_val_X.drop('meter_reading_log1p', axis=1), f_val_X['meter_reading_log1p'])],\n",
    "        # https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114722#latest-660848\n",
    "        # eval_metric=lbm_rmslee,\n",
    "        early_stopping_rounds=20)\n",
    "    models.append(gbm)\n",
    "    best_scores.append(gbm.best_score_)\n",
    "    del f_train_X, f_val_X, gbm\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scores for cross val\n",
    "for score in best_scores:\n",
    "    print(score['valid_0']['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importance rank for first model in cross val models\n",
    "imprtc_df = pd.DataFrame()\n",
    "imprtc_df['feature'] = sample_train_X.drop('meter_reading_log1p', axis=1).columns   \n",
    "imprtc_df['importance'] = models[0].feature_importances_\n",
    "imprtc_df.sort_values('importance', ascending=False, inplace= True)\n",
    "print(imprtc_df)\n",
    "print(sample_train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
