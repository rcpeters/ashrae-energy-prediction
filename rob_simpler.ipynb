{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, TransformerMixin\n",
    "import gc\n",
    "from os import path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.core.dtypes.dtypes import CategoricalDtype\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "# label encoding\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dtype = {\n",
    "    'train': {'building_id': np.int16, 'meter': np.int8, 'meter_reading': np.float32},\n",
    "    'test': {'building_id': np.int16, 'meter': np.int8},\n",
    "    'building_metadata': {'site_id': np.int8, 'building_id': np.uint16, 'square_feet': np.int32, 'year_built': np.float32, 'floor_count': np.float32},\n",
    "    'weather' : {'site_id': np.int8, 'air_temperature': np.float32, 'cloud_coverage': np.float32, 'dew_temperature': np.float32,\n",
    "                     'precip_depth_1_hr': np.float32, 'sea_level_pressure': np.float32, 'wind_direction': np.float32, 'wind_speed': np.float32}\n",
    "}\n",
    "\n",
    "file_loc = {}    \n",
    "for dir_path in ['../input/ashrae-energy-prediction/','../input/_ashrae-energy-prediction/']:\n",
    "    for name in ['building_metadata','weather_train','weather_test','train','test']:\n",
    "        if path.exists(dir_path + name + '.csv'):\n",
    "            file_loc[name]= dir_path + name + '.csv'\n",
    "    \n",
    "    building = pd.read_csv(file_loc['building_metadata'], dtype=file_dtype['building_metadata'])\n",
    "    weather_train = pd.read_csv(file_loc['weather_train'], dtype=file_dtype['weather'])\n",
    "    weather_test = pd.read_csv(file_loc['weather_test'], dtype=file_dtype['weather'])\n",
    "    train = pd.read_csv(file_loc['train'], dtype=file_dtype['train'])\n",
    "    test = pd.read_csv(file_loc['test'], dtype=file_dtype['test'])\n",
    "\n",
    "train = train.merge(building, on='building_id', how='left')\n",
    "test = test.merge(building, on='building_id', how='left')\n",
    "train = train.merge(weather_train, on=['site_id', 'timestamp'], how='left')\n",
    "test = test.merge(weather_test, on=['site_id', 'timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cloud_coverage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7062.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6030.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4298.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3458.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2354.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cloud_coverage\n",
       "site_id                \n",
       "7                8614.0\n",
       "11               8614.0\n",
       "1                7062.0\n",
       "5                6030.0\n",
       "15               4399.0\n",
       "13               4298.0\n",
       "4                4230.0\n",
       "0                3830.0\n",
       "8                3830.0\n",
       "3                3642.0\n",
       "9                3458.0\n",
       "14               3311.0\n",
       "6                2992.0\n",
       "10               2450.0\n",
       "2                2354.0\n",
       "12                 59.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(weather_train.groupby(['site_id','cloud_coverage'])['site_id','cloud_coverage'].transform('sum'))\n",
    "weather_train.groupby(['site_id']).agg({'cloud_coverage': lambda x: x.isnull().sum()}).sort_values('cloud_coverage', ascending=False)\n",
    "\n",
    "# note some building are built in the future!\n",
    "#train[ train['year_built'] > 2000]['year_built'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDatetime(df):\n",
    "    #time_stamps\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    print('-- convertToDatetime done---------------------------')\n",
    "    print(df[[\"timestamp\"]].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features we are 100% sure about\n",
    "def logSquareFeet(df):\n",
    "    df.rename(columns={\"square_feet\": \"log_square_feet\"}, inplace=True)\n",
    "    df['log_square_feet'] = np.float16(np.log(df['log_square_feet']))\n",
    "    print('-- logSquareFeet done---------------------------')\n",
    "    print(df[[\"log_square_feet\"]].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set types category types\n",
    "def setCatTypes(df):\n",
    "    df[\"primary_use\"]= df[\"primary_use\"].astype(\"category\")\n",
    "    df[\"meter\"] = df[\"meter\"].astype(\"category\")\n",
    "    df[\"site_id\"] = df[\"site_id\"].astype(\"category\")\n",
    "    df[\"building_id\"] = df[\"building_id\"].astype(\"category\")\n",
    "    print('-- setCatTypes done---------------------------')\n",
    "    print(df[[\"primary_use\",\"meter\",\"site_id\",\"building_id\"]].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing cloud coverage\n",
    "def imputeCloudCoverage(df):\n",
    "    # set age of building to mediam of site_id\n",
    "    # else if set ot overall median\n",
    "    median = df['cloud_coverage'].median()\n",
    "    # Set all year_built NaNs to site mean for year_built\n",
    "    for i, i_median in df.groupby(['site_id'])['cloud_coverage'].median().items():\n",
    "        print(str(i) + \" \" +str(i_median))\n",
    "        #print(i_median)\n",
    "        if not np.isnan(i_median):\n",
    "            df.loc[(df['cloud_coverage'].isnull()) & (df['site_id'] == i), 'cloud_coverage'] = i_median\n",
    "        else:\n",
    "            df.loc[(df['cloud_coverage'].isnull()) & (df['site_id'] == i), 'cloud_coverage'] = median\n",
    "    df['cloud_coverage'] = np.uint8(df['cloud_coverage'])\n",
    "    print('-- impute year built done---------------------------')\n",
    "    print(df.groupby(['site_id'])['cloud_coverage'].describe())\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates bucked categories for cloud coverage by time day\n",
    "# t[clound converage /2]c[hour divied /4]\n",
    "def cloudTimeCat(df):\n",
    "    tempDf = df[['cloud_coverage', 'hour']].astype('int')\n",
    "    tempDf['cloud_coverage'] = (tempDf['cloud_coverage']).astype('int')\n",
    "    tempDf['hour'] = (tempDf['hour']).astype('int')\n",
    "    tempDf = tempDf.astype('str')\n",
    "    df['cloud_time_cat'] = 'c' + tempDf['cloud_coverage'] + 't' + tempDf['hour']\n",
    "    df['cloud_time_cat'] = df['cloud_time_cat'].astype('category')\n",
    "    print('-- cloudHourCat done---------------------------')\n",
    "    print(df[['cloud_time_cat']].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')                                                                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropCols(TransformerMixin):\n",
    "\n",
    "    def __init__(self, drop_cols):\n",
    "        self._drop_cols = drop_cols\n",
    "        \n",
    "    def transform(self, df, **transform_params):\n",
    "        return df.drop(self._drop_cols, axis=1)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating building_age\n",
    "def imputeYearBuilt(df):\n",
    "    # set age of building to mediam of site_id\n",
    "    # else if set ot overall median\n",
    "    year_built_median = df['year_built'].median()\n",
    "    # Set all year_built NaNs to site mean for year_built\n",
    "    for i, i_median in df.groupby(['site_id'])['year_built'].median().items():\n",
    "        if not np.isnan(i_median):\n",
    "            df.loc[(df['year_built'].isnull()) & (df['site_id'] == i), 'year_built'] = i_median\n",
    "        else:\n",
    "            df.loc[(df['year_built'].isnull()) & (df['site_id'] == i), 'year_built'] = year_built_median\n",
    "    df['building_age'] = np.uint8(df['year_built']-1900)\n",
    "    print('-- impute year built done---------------------------')\n",
    "    print(df.groupby(['site_id'])['building_age'].describe())\n",
    "    print('\\n\\n')\n",
    "    \n",
    "class ImputeYearBuilt(TransformerMixin):\n",
    "\n",
    "    def transform(self, df, **transform_params):\n",
    "        copy = df\n",
    "        year_built_median = copy['year_built'].median()\n",
    "        # Set all year_built NaNs to site mean for year_built\n",
    "        for i, i_median in copy.groupby(['site_id'])['year_built'].median().items():\n",
    "            if not np.isnan(i_median):\n",
    "                copy.loc[(copy['year_built'].isnull()) & (copy['site_id'] == i), 'year_built'] = i_median\n",
    "            else:\n",
    "                copy.loc[(copy['year_built'].isnull()) & (copy['site_id'] == i), 'year_built'] = year_built_median\n",
    "        copy['building_age'] = np.uint8(copy['year_built']-1900)\n",
    "        return copy\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMeterDummies(df):\n",
    "    for i in range(4):\n",
    "        df[\"_meter_\"+str(i)] = (df['building_id'].isin(\n",
    "            train.loc[train['meter'] == i].building_id.unique()))\n",
    "    print('-- addMeterDummies done---------------------------')\n",
    "    print(df[['_meter_0','_meter_1','_meter_2','_meter_3']].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTimeFeatures(df):\n",
    "    df['dayofweek'] = df[\"timestamp\"].dt.dayofweek.astype('category') # vs weekend?\n",
    "    #df['weekday'] = df[\"timestamp\"].dt.weekday.astype('category')\n",
    "    df[\"week\"] = df[\"timestamp\"].dt.week.astype('category')\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype('category')\n",
    "    print('-- addTimeFeatures done---------------------------')\n",
    "    print(df['timestamp'].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRelativeHumidity(df):\n",
    "    # placeholder http://bmcnoldy.rsmas.miami.edu/Humidity.html\n",
    "    print('-- addRelativeHumidity done---------------------------')\n",
    "    #print(df['relative_humidity'].sample(n=20, random_state=42))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- convertToDatetime done---------------------------\n",
      "                   timestamp\n",
      "14245562 2016-09-16 16:00:00\n",
      "1282718  2016-01-24 06:00:00\n",
      "13883790 2016-09-10 07:00:00\n",
      "4781820  2016-04-01 01:00:00\n",
      "10415393 2016-07-10 04:00:00\n",
      "1057008  2016-01-20 04:00:00\n",
      "4507399  2016-03-26 20:00:00\n",
      "19478829 2016-12-18 23:00:00\n",
      "8955615  2016-06-14 06:00:00\n",
      "13799839 2016-09-08 19:00:00\n",
      "15647011 2016-10-11 11:00:00\n",
      "2524294  2016-02-16 08:00:00\n",
      "10016102 2016-07-03 02:00:00\n",
      "3915750  2016-03-15 03:00:00\n",
      "17217526 2016-11-08 09:00:00\n",
      "11478    2016-01-01 04:00:00\n",
      "18919011 2016-12-09 02:00:00\n",
      "8709341  2016-06-09 21:00:00\n",
      "16313567 2016-10-23 07:00:00\n",
      "6289526  2016-04-27 20:00:00\n",
      "\n",
      "\n",
      "\n",
      "-- addRelativeHumidity done---------------------------\n",
      "\n",
      "\n",
      "\n",
      "-- addTimeFeatures done---------------------------\n",
      "14245562   2016-09-16 16:00:00\n",
      "1282718    2016-01-24 06:00:00\n",
      "13883790   2016-09-10 07:00:00\n",
      "4781820    2016-04-01 01:00:00\n",
      "10415393   2016-07-10 04:00:00\n",
      "1057008    2016-01-20 04:00:00\n",
      "4507399    2016-03-26 20:00:00\n",
      "19478829   2016-12-18 23:00:00\n",
      "8955615    2016-06-14 06:00:00\n",
      "13799839   2016-09-08 19:00:00\n",
      "15647011   2016-10-11 11:00:00\n",
      "2524294    2016-02-16 08:00:00\n",
      "10016102   2016-07-03 02:00:00\n",
      "3915750    2016-03-15 03:00:00\n",
      "17217526   2016-11-08 09:00:00\n",
      "11478      2016-01-01 04:00:00\n",
      "18919011   2016-12-09 02:00:00\n",
      "8709341    2016-06-09 21:00:00\n",
      "16313567   2016-10-23 07:00:00\n",
      "6289526    2016-04-27 20:00:00\n",
      "Name: timestamp, dtype: datetime64[ns]\n",
      "\n",
      "\n",
      "\n",
      "-- logSquareFeet done---------------------------\n",
      "          log_square_feet\n",
      "14245562        11.343750\n",
      "1282718         11.117188\n",
      "13883790        11.851562\n",
      "4781820         12.554688\n",
      "10415393        10.406250\n",
      "1057008         12.437500\n",
      "4507399         12.390625\n",
      "19478829        10.312500\n",
      "8955615         11.687500\n",
      "13799839        12.414062\n",
      "15647011        12.171875\n",
      "2524294          8.632812\n",
      "10016102         6.625000\n",
      "3915750         11.078125\n",
      "17217526        12.531250\n",
      "11478           11.507812\n",
      "18919011        11.335938\n",
      "8709341          8.625000\n",
      "16313567        11.015625\n",
      "6289526         10.828125\n",
      "\n",
      "\n",
      "\n",
      "-- impute year built done---------------------------\n",
      "             count       mean        std   min   25%    50%    75%    max\n",
      "site_id                                                                  \n",
      "0        1076662.0  96.309126  14.322800  68.0  85.0  101.0  107.0  116.0\n",
      "1         553357.0  60.398303  28.233491   0.0  56.0   60.0   70.0  107.0\n",
      "2        2530312.0  72.958444  21.578982   7.0  64.0   70.0   88.0  114.0\n",
      "3        2370097.0  61.244291  24.810568   0.0  61.0   61.0   61.0  117.0\n",
      "4         746746.0  53.378046  31.688006   3.0  24.0   54.0   70.0  116.0\n",
      "5         781776.0  62.752809  22.077142  19.0  66.0   76.0   76.0  113.0\n",
      "6         668133.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "7         366681.0  61.468571  18.794832  11.0  55.0   64.0   70.0   95.0\n",
      "8         567915.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "9        2679323.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "10        411407.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "11        119459.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "12        315909.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "13       2711763.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "14       2501506.0  69.000000   0.000000  69.0  69.0   69.0   69.0   69.0\n",
      "15       1815054.0  61.011679  29.221940   2.0  46.0   63.0   81.0  114.0\n",
      "\n",
      "\n",
      "\n",
      "0 2.0\n",
      "1 0.0\n",
      "2 2.0\n",
      "3 4.0\n",
      "4 2.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 nan\n",
      "8 2.0\n",
      "9 0.0\n",
      "10 0.0\n",
      "11 nan\n",
      "12 7.0\n",
      "13 2.0\n",
      "14 0.0\n",
      "15 0.0\n",
      "-- impute year built done---------------------------\n",
      "             count      mean       std  min  25%  50%  75%  max\n",
      "site_id                                                        \n",
      "0        1076662.0  2.593296  1.665471  0.0  2.0  2.0  4.0  9.0\n",
      "1         553357.0  0.016394  0.383773  0.0  0.0  0.0  0.0  9.0\n",
      "2        2530312.0  1.969933  1.637573  0.0  0.0  2.0  2.0  9.0\n",
      "3        2370097.0  3.929091  2.090933  0.0  4.0  4.0  4.0  9.0\n",
      "4         746746.0  1.928517  0.981933  0.0  2.0  2.0  2.0  9.0\n",
      "5         781776.0  0.090164  0.896296  0.0  0.0  0.0  0.0  9.0\n",
      "6         668133.0  0.424836  1.346262  0.0  0.0  0.0  0.0  9.0\n",
      "7         366681.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n",
      "8         567915.0  2.597922  1.676015  0.0  2.0  2.0  4.0  9.0\n",
      "9        2679323.0  0.370624  1.083932  0.0  0.0  0.0  0.0  9.0\n",
      "10        411407.0  0.272543  1.145751  0.0  0.0  0.0  0.0  9.0\n",
      "11        119459.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n",
      "12        315909.0  5.735772  2.234671  0.0  4.0  7.0  7.0  8.0\n",
      "13       2711763.0  2.445887  1.987124  0.0  2.0  2.0  2.0  9.0\n",
      "14       2501506.0  0.349933  1.199240  0.0  0.0  0.0  0.0  9.0\n",
      "15       1815054.0  0.538218  1.277137  0.0  0.0  0.0  0.0  9.0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fill_w_neg_one = []\n",
    "fill_w_zero = ['floor_count']\n",
    "fill_w_popular = []\n",
    "fill_w_mean = ['air_temperature','dew_temperature', \n",
    "              \"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_speed\"]\n",
    "\n",
    "def generalImputes(df):\n",
    "    for col in fill_w_neg_one:\n",
    "        df[col].fillna(-1, inplace=True)\n",
    "    for col in fill_w_popular:\n",
    "        df[col].fillna(df[col].value_counts()[0], inplace=True)\n",
    "    for col in fill_w_zero:\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    for col in fill_w_mean:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "    print(df[fill_w_neg_one + fill_w_zero + fill_w_popular + fill_w_mean].sample(n=20, random_state=42))\n",
    "    \n",
    "            \n",
    "for df in [train, test]:\n",
    "    convertToDatetime(df)\n",
    "    addRelativeHumidity(df)\n",
    "    addTimeFeatures(df)\n",
    "    logSquareFeet(df)\n",
    "    imputeYearBuilt(df)\n",
    "    imputeCloudCoverage(df)\n",
    "    cloudTimeCat(df) #this feature ranks high but doesn't change the score \n",
    "    addMeterDummies(df)\n",
    "    generalImputes(df)\n",
    "    setCatTypes(df)\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('--NaN Checks')\n",
    "print(train.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_cols = ['row_id','wind_direction','year_built','meter_reading','timestamp','precip_depth_1_hr']  \n",
    "# create test train\n",
    "#train_y =  np.log1p(train[\"meter_reading\"]) # ask why\n",
    "#train_X = train.drop(filter(lambda i: i!='row_id', drop_cols), axis=1)\n",
    "#test_X = test.drop(filter(lambda i: i!='meter_reading', drop_cols), axis=1)\n",
    "\n",
    "\n",
    "gc.collect();\n",
    "\n",
    "print(train.dtypes)\n",
    "print(train.columns)\n",
    "print(test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return np.sqrt(mean_squared_log_error(y, y_pred.clip(0)))\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return mean_squared_error(y, y_pred.clip(0))\n",
    "\n",
    "def rmsee(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y.clip(0)), np.expm1(y_pred.clip(0))))\n",
    "    \n",
    "rmsle_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsle(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsle(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "rmsee_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsee(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "\n",
    "gbm=LGBMRegressor(n_estimators=100, # for accuracy use large numbers like 6000 \n",
    "                  learning_rate=0.23,\n",
    "                  feature_fraction=0.9,\n",
    "                  subsample=0.1,  # batches of 20% of the data\n",
    "                  subsample_freq=1,\n",
    "                  num_leaves=20,\n",
    "                  verbose= 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['wind_direction','year_built','timestamp','precip_depth_1_hr']  \n",
    "\n",
    "union_pipe = FeatureUnion (transformer_list = [\n",
    "    ('imputeYearBuilt', ImputeYearBuilt())\n",
    "])\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        #('union_pipe', union_pipe),\n",
    "        ('dropClos', DropCols(drop_cols)),\n",
    "        ('gbm', gbm)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-06c8ceed8cdf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                      n_jobs=-1)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mgd_sr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'meter_reading'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meter_reading\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgd_sr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(type(np.log1p(train[\"meter_reading\"])))\n",
    "#print(train[['meter_reading']])\n",
    "# Cross val testing - can be skipped\n",
    "#scores = cross_val_score(gbm, train_X, train_y, cv=3, \n",
    "#                         scoring=rmsee_scorer)\n",
    "#print(\"rmsee scores:\\n\", scores)\n",
    "\n",
    "grid_param = {\n",
    "    #'n_estimators': [1000, 3000, 6000],\n",
    "    'gbm__subsample': [0.1]\n",
    "}\n",
    "\n",
    "gd_sr = GridSearchCV(pipe,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring=rmsee_scorer,\n",
    "                     cv=3,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(train.drop('meter_reading', axis=1), np.log1p(train[[\"meter_reading\"]]))\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "print(best_parameters)\n",
    "best_result = gd_sr.best_score_\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit on all the data\n",
    "pipe.fit(\n",
    "    train.drop('meter_reading', axis=1), \n",
    "    np.log1p(train[[\"meter_reading\"]]),\n",
    "    gbm__eval_metric=rmsee, gbm__verbose=100\n",
    ")\n",
    "\n",
    "#gbm.fit(train_X, train_y, eval_metric=rmsee, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imprtc_df = pd.DataFrame()\n",
    "imprtc_df[\"feature\"] = train_X.columns   \n",
    "imprtc_df[\"importance\"] = pipe.named_steps['gbm'].feature_importances_\n",
    "imprtc_df.sort_values('importance', ascending=False, inplace= True)\n",
    "print(imprtc_df)\n",
    "print(imprtc_df.nsmallest(7,'importance')['feature'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect();\n",
    "test_y = pipe.predict(test.drop('row_id', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect();\n",
    "\n",
    "'''\n",
    "test_X = test.drop('row_id'), axis=1)\n",
    "\n",
    "from tqdm import tqdm\n",
    "i=0\n",
    "res=[]\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test_X.shape[0]/50000)))):\n",
    "    res.append(gbm.predict(test_X.iloc[i:i+step_size]))\n",
    "    i+=step_size\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/ashrae-energy-prediction/sample_submission.csv\")\n",
    "res = np.concatenate(res)\n",
    "# hack to prevent negative numbers\n",
    "sub[\"meter_reading\"] = np.expm1(test_y.clip(0))\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
