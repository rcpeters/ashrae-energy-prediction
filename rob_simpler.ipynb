{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import - second run you can skip\n",
    "building = pd.read_csv('../input/ashrae-energy-prediction/building_metadata.csv')\n",
    "weather_train = pd.read_csv('../input/ashrae-energy-prediction/weather_train.csv')\n",
    "weather_test = pd.read_csv('../input/ashrae-energy-prediction/weather_test.csv')\n",
    "train = pd.read_csv('../input/ashrae-energy-prediction/train.csv')\n",
    "test = pd.read_csv('../input/ashrae-energy-prediction/test.csv')\n",
    "\n",
    "train = train.merge(building, on='building_id', how='left')\n",
    "test = test.merge(building, on='building_id', how='left')\n",
    "train = train.merge(weather_train, on=['site_id', 'timestamp'], how='left')\n",
    "test = test.merge(weather_test, on=['site_id', 'timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pickle - second run you can skip\n",
    "mergePickle = {\n",
    "    'train': train,\n",
    "    'test': test\n",
    "}\n",
    "\n",
    "del weather_train, weather_test,building\n",
    "gc.collect();\n",
    "pickle.dump(mergePickle, open( \"mergePickle.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip to here if you run the previous\n",
    "mergePickle = pickle.load( open( \"mergePickle.p\", \"rb\" ) )\n",
    "train = mergePickle['train']\n",
    "test = mergePickle['test']\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id           0\n",
      "meter                 0\n",
      "timestamp             0\n",
      "meter_reading         0\n",
      "site_id               0\n",
      "primary_use           0\n",
      "square_feet           0\n",
      "year_built            0\n",
      "floor_count           0\n",
      "air_temperature       0\n",
      "cloud_coverage        0\n",
      "dew_temperature       0\n",
      "precip_depth_1_hr     0\n",
      "sea_level_pressure    0\n",
      "wind_direction        0\n",
      "wind_speed            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill NaNs\n",
    "fill_w_neg_one = ['site_id']\n",
    "fill_w_zero = ['floor_count']\n",
    "fill_w_popular = ['primary_use']\n",
    "fill_w_mean = ['cloud_coverage','year_built','air_temperature','dew_temperature', \n",
    "              \"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\n",
    "\n",
    "for df in [train, test]:\n",
    "    for col in fill_w_neg_one:\n",
    "        df[col].fillna(-1, inplace=True)\n",
    "    for col in fill_w_popular:\n",
    "        df[col].fillna(df[col].value_counts()[0], inplace=True)\n",
    "    for col in fill_w_zero:\n",
    "        df[col].fillna(0, inplace=True)\n",
    "    for col in fill_w_mean:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  6,  1,  7, 11,  8,  9, 15,  2, 10,  3, 14, 13,  5, 12],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoding\n",
    "le = LabelEncoder()\n",
    "\n",
    "for df in [train, test]:\n",
    "    df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n",
    "    \n",
    "train[\"primary_use\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_id           category\n",
      "meter                 category\n",
      "timestamp               object\n",
      "meter_reading          float64\n",
      "site_id               category\n",
      "primary_use           category\n",
      "square_feet              int32\n",
      "year_built             float16\n",
      "floor_count           category\n",
      "air_temperature        float32\n",
      "cloud_coverage         float16\n",
      "dew_temperature        float32\n",
      "precip_depth_1_hr      float16\n",
      "sea_level_pressure     float32\n",
      "wind_direction         float16\n",
      "wind_speed             float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Saving some memory setting types\n",
    "d_types = {'building_id': 'category', #np.int16,\n",
    "          'meter': 'category', # np.int8,\n",
    "          'site_id': 'category', #np.int8,\n",
    "          'primary_use': 'category',\n",
    "          'floor_count': 'category',\n",
    "          'square_feet': np.int32,\n",
    "          'year_built': np.float16,\n",
    "          'air_temperature': np.float32,\n",
    "          'cloud_coverage': np.float16,\n",
    "          'dew_temperature': np.float32,\n",
    "          'precip_depth_1_hr': np.float16,\n",
    "          'sea_level_pressure': np.float32,\n",
    "          'wind_direction': np.float16,\n",
    "          'wind_speed': np.float32}\n",
    "\n",
    "for df in [train, test]:\n",
    "    for feature in d_types: \n",
    "        df[feature] = df[feature].astype(d_types[feature])\n",
    "    \n",
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "drop_cols = ['wind_direction']\n",
    "for df in [train, test]: \n",
    "    df = df.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add non-imputed features \n",
    "def nonIFeatures(df):\n",
    "    #time_stamps\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df['dayofweek'] = df[\"timestamp\"].dt.dayofweek.astype('category') # vs weekend?\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype('category')\n",
    "    #df[\"month\"] = df[\"timestamp\"].dt.month.astype('category')\n",
    "    \n",
    "    # each row should know about other meters \n",
    "    for i in range(4):\n",
    "        df[\"_meter_\"+str(i)] = df['building_id'].isin(\n",
    "            train.loc[train['meter'] == i].building_id.unique()).astype('category')\n",
    "    \n",
    "    df.rename(columns={\"square_feet\": \"log_square_feet\"}, inplace=True)\n",
    "    df['log_square_feet'] = np.float16(np.log(df['log_square_feet']))\n",
    "    df['year_built'] = np.uint8(df['year_built']-1900)    \n",
    "    \n",
    "for df in [train, test]:\n",
    "    df = nonIFeatures(df)\n",
    "    \n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test train\n",
    "train_y =  np.log1p(train[\"meter_reading\"]) # ask why\n",
    "train_X = train.drop([\"meter_reading\",\"timestamp\"], axis=1)\n",
    "test_X = test.drop([\"row_id\",\"timestamp\"], axis=1)\n",
    "\n",
    "gc.collect();\n",
    "\n",
    "print(train_X.columns)\n",
    "train.loc[train['meter_reading'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsle scores:\n",
      " [-0.50787072 -0.46765246 -0.38312615 -0.40704553 -0.42841516]\n"
     ]
    }
   ],
   "source": [
    "# Rob hacking\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return np.sqrt(mean_squared_log_error(y, y_pred.clip(0)))\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    # hack to prevent negative numbers\n",
    "    return mean_squared_error(y, y_pred.clip(0))\n",
    "\n",
    "rmsle_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsle(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "rmse_scorer = make_scorer(\n",
    "    lambda y_true, y_pred : rmsle(y_true, y_pred), \n",
    "    greater_is_better=False)\n",
    "\n",
    "gbm=LGBMRegressor( task= 'train',\n",
    "                  boosting_type= 'gbdt',\n",
    "                  objective= 'regression',\n",
    "                  #n_estimators=6000,\n",
    "                  learning_rate= 0.09,\n",
    "                  feature_fraction= 0.9,\n",
    "                  bagging_fraction= 0.9,\n",
    "                  subsample=0.2,  # batches of 20% of the data\n",
    "                  subsample_freq=1,\n",
    "                  num_leaves=20,\n",
    "                  verbose= 100)\n",
    "\n",
    "scores = cross_val_score(gbm, train_X, train_y, cv=5, \n",
    "                         scoring=rmse_scorer)\n",
    "print(\"rmsle scores:\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(bagging_fraction=0.9, boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=1.0, feature_fraction=0.9,\n",
       "              importance_type='split', learning_rate=0.09, max_depth=-1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=20,\n",
       "              objective='regression', random_state=None, reg_alpha=0.0,\n",
       "              reg_lambda=0.0, silent=True, subsample=0.2,\n",
       "              subsample_for_bin=200000, subsample_freq=1, task='train',\n",
       "              verbose=100)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(train_X, train_y, eval_metric=rmsl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1149  249  100   13   62    3   10  128    1   86    0   18    2   28\n",
      "   50    0    0    0    1]\n",
      "               feature  importance\n",
      "0          building_id        1149\n",
      "1                meter         249\n",
      "7      air_temperature         128\n",
      "2              site_id         100\n",
      "9      dew_temperature          86\n",
      "4      log_square_feet          62\n",
      "14                hour          50\n",
      "13           dayofweek          28\n",
      "11  sea_level_pressure          18\n",
      "3          primary_use          13\n",
      "6          floor_count          10\n",
      "5           year_built           3\n",
      "12          wind_speed           2\n",
      "8       cloud_coverage           1\n",
      "18            _meter_3           1\n",
      "10   precip_depth_1_hr           0\n",
      "15            _meter_0           0\n",
      "16            _meter_1           0\n",
      "17            _meter_2           0\n"
     ]
    }
   ],
   "source": [
    "print( gbm.feature_importances_)\n",
    "imprtc_df = pd.DataFrame()\n",
    "imprtc_df[\"feature\"] = train_X.columns   \n",
    "imprtc_df[\"importance\"] = gbm.feature_importances_\n",
    "print(imprtc_df.sort_values('importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 834/834 [01:51<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i=0\n",
    "res=[]\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test_X.shape[0]/50000)))):\n",
    "    res.append(gbm.predict(test_X.iloc[i:i+step_size]))\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"../input/ashrae-energy-prediction/sample_submission.csv\")\n",
    "res = np.concatenate(res)\n",
    "# hack to prevent negative numbers\n",
    "sub[\"meter_reading\"] = np.expm1(res.clip(0))\n",
    "sub.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
